
@article{viola_adjusted_2020,
	title = {An Adjusted Nearest Neighbor Algorithm Maximizing the F-Measure from Imbalanced Data},
	url = {http://arxiv.org/abs/1909.00693},
	abstract = {In this paper, we address the challenging problem of learning from imbalanced data using a Nearest-Neighbor ({NN}) algorithm. In this setting, the minority examples typically belong to the class of interest requiring the optimization of speciﬁc criteria, like the F-Measure. Based on simple geometrical ideas, we introduce an algorithm that reweights the distance between a query sample and any positive training example. This leads to a modiﬁcation of the Voronoi regions and thus of the decision boundaries of the {NN} algorithm. We provide a theoretical justiﬁcation about the weighting scheme needed to reduce the False Negative rate while controlling the number of False Positives. We perform an extensive experimental study on many public imbalanced datasets, but also on large scale non public data from the French Ministry of Economy and Finance on a tax fraud detection task, showing that our method is very eﬀective and, interestingly, yields the best performance when combined with state of the art sampling methods.},
	journaltitle = {{arXiv}:1909.00693 [cs, stat]},
	author = {Viola, Rémi and Emonet, Rémi and Habrard, Amaury and Metzler, Guillaume and Riou, Sébastien and Sebban, Marc},
	urldate = {2021-04-19},
	date = {2020-01},
	langid = {english},
	keywords = {focused},
	annotation = {{arXiv}: 1909.00693},
	annotation = {Comment: In Proceedings of the 31 International Conference on Tools with Artificial Intelligence ({ICTAI} 2019)},
	file = {1909.00693.pdf:C\:\\Users\\Celian\\Zotero\\storage\\QEF8B3BF\\1909.00693.pdf:application/pdf}
}

@book{aggarwal_outlier_2016,
	edition = {2nd},
	title = {Outlier Analysis},
	isbn = {978-3-319-47577-6},
	abstract = {This book provides comprehensive coverage of the field of outlier analysis from a computer science point of view. It integrates methods from data mining, machine learning, and statistics within the computational framework and therefore appeals to multiple communities. The chapters of this book can be organized into three categories:Basic algorithms: Chapters 1 through 7 discuss the fundamental algorithms for outlier analysis, including probabilistic and statistical methods, linear methods, proximity-based methods, high-dimensional (subspace) methods, ensemble methods, and supervised methods. Domain-specific methods: Chapters 8 through 12 discuss outlier detection algorithms for various domains of data, such as text, categorical data, time-series data, discrete sequence data, spatial data, and network data. Applications: Chapter 13 is devoted to various applications of outlier analysis. Some guidance is also provided for the practitioner. The second edition of this book is more detailed and is written to appeal to both researchers and practitioners. Significant new material has been added on topics such as kernel methods, one-class support-vector machines, matrix factorization, neural networks, outlier ensembles, time-series methods, and subspace methods. It is written as a textbook and can be used for classroom teaching.},
	publisher = {Springer Publishing Company, Incorporated},
	author = {Aggarwal, Charu C.},
	date = {2016},
	keywords = {[introduction], imbalanced data : {SOA}, outlier detection : {SOA}}
}

@article{barandela_strategies_2003,
	title = {Strategies for learning in class imbalance problems},
	pages = {4},
	journaltitle = {Pattern Recognition},
	author = {Barandela, R and {SaÃnchez}, J S and {GarcÃa}, V and Rangel, E},
	date = {2003},
	langid = {english},
	keywords = {[experiment], algo : cwk-nn, distance methods : geometric dist, eval mesure : {ROC}, sampling method : downsizing},
	file = {Barandela et al. - 2003 - Strategies for learning in class imbalance problem.pdf:C\:\\Users\\Celian\\Zotero\\storage\\DMALA3GE\\Barandela et al. - 2003 - Strategies for learning in class imbalance problem.pdf:application/pdf}
}

@article{hasanin_severely_2019,
	title = {Severely imbalanced Big Data challenges: investigating data sampling approaches},
	volume = {6},
	issn = {2196-1115},
	url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0274-4},
	doi = {10.1186/s40537-019-0274-4},
	shorttitle = {Severely imbalanced Big Data challenges},
	abstract = {Severe class imbalance between majority and minority classes in Big Data can bias the predictive performance of Machine Learning algorithms toward the majority (negative) class. Where the minority (positive) class holds greater value than the majority (negative) class and the occurrence of false negatives incurs a greater penalty than false positives, the bias may lead to adverse consequences. Our paper incorporates two case studies, each utilizing three learners, six sampling approaches, two performance metrics, and five sampled distribution ratios, to uniquely investigate the effect of severe class imbalance on Big Data analytics. The learners (Gradient-Boosted Trees, Logistic Regression, Random Forest) were implemented within the Apache Spark framework. The first case study is based on a Medicare fraud detection dataset. The second case study, unlike the first, includes training data from one source ({SlowlorisBig} Dataset) and test data from a separate source ({POST} dataset). Results from the Medicare case study are not conclusive regarding the best sampling approach using Area Under the Receiver Operating Characteristic Curve and Geometric Mean performance metrics. However, it should be noted that the Random Undersampling approach performs adequately in the first case study. For the {SlowlorisBig} case study, Random Undersampling convincingly outperforms the other five sampling approaches (Random Oversampling, Synthetic Minority Over-sampling {TEchnique}, {SMOTE}-borderline1 , {SMOTE}-borderline2 , {ADAptive} {SYNthetic}) when measuring performance with Area Under the Receiver Operating Characteristic Curve and Geometric Mean metrics. Based on its classification performance in both case studies, Random Undersampling is the best choice as it results in models with a significantly smaller number of samples, thus reducing computational burden and training time.},
	pages = {107},
	number = {1},
	journaltitle = {Journal of Big Data},
	author = {Hasanin, Tawfiq and Khoshgoftaar, Taghi M. and Leevy, Joffrey L. and Bauder, Richard A.},
	urldate = {2021-04-19},
	date = {2019-12},
	langid = {english},
	keywords = {[introduction], anomaly detection : {SOA}, imbalanced data : {SOA}, sampling method : {SOA}},
	annotation = {Pas exactement le papier introduit dans l'article},
	file = {Hasanin et al. - 2019 - Severely imbalanced Big Data challenges investiga.pdf:C\:\\Users\\Celian\\Zotero\\storage\\P97D3FMX\\Hasanin et al. - 2019 - Severely imbalanced Big Data challenges investiga.pdf:application/pdf}
}

@book{bellet_metric_2015,
	title = {Metric Learning},
	volume = {9},
	url = {https://hal.archives-ouvertes.fr/hal-01121733},
	series = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
	abstract = {Similarity between objects plays an important role in both human cognitive processes and artificial systems for recognition and categorization. How to appropriately measure such similarities for a given task is crucial to the performance of many machine learning, pattern recognition and data mining methods. This book is devoted to metric learning, a set of techniques to automatically learn similarity and distance functions from data that has attracted a lot of interest in machine learning and related fields in the past ten years. In this book, we provide a thorough review of the metric learning literature that covers algorithms, theory and applications for both numerical and structured data. We first introduce relevant definitions and classic metric functions, as well as examples of their use in machine learning and data mining. We then review a wide range of metric learning algorithms, starting with the simple setting of linear distance and similarity learning. We show how one may scale-up these methods to very large amounts of training data. To go beyond the linear case, we discuss methods that learn nonlinear metrics or multiple linear metrics throughout the feature space, and review methods for more complex settings such as multi-task and semi-supervised learning. Although most of the existing work has focused on numerical data, we cover the literature on metric learning for structured data like strings, trees, graphs and time series. In the more technical part of the book, we present some recent statistical frameworks for analyzing the generalization performance in metric learning and derive results for some of the algorithms presented earlier. Finally, we illustrate the relevance of metric learning in real-world problems through a series of successful applications to computer vision, bioinformatics and information retrieval.},
	number = {1},
	publisher = {Morgan \& Claypool Publishers ({USA}), Synthesis Lectures on Artificial Intelligence and Machine Learning, pp 1-151},
	author = {Bellet, Aurélien and Habrard, Amaury and Sebban, Marc},
	urldate = {2021-04-19},
	date = {2015-01},
	doi = {10.2200/S00626ED1V01Y201501AIM030},
	keywords = {[introduction], distance methods : {SOA}, metric learning},
	file = {HAL Snapshot:C\:\\Users\\Celian\\Zotero\\storage\\FNBH3YFE\\hal-01121733.html:text/html}
}

@article{chandola_anomaly_2009,
	title = {Anomaly detection: A survey},
	volume = {41},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/1541880.1541882},
	doi = {10.1145/1541880.1541882},
	shorttitle = {Anomaly detection},
	abstract = {Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and more succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.},
	pages = {15:1--15:58},
	number = {3},
	journaltitle = {{ACM} Computing Surveys},
	author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
	urldate = {2021-04-19},
	date = {2009-07},
	keywords = {[introduction], anomaly detection : {SOA}}
}

@article{cover_nearest_1967,
	title = {Nearest neighbor pattern classification},
	volume = {13},
	issn = {1557-9654},
	doi = {10.1109/TIT.1967.1053964},
	abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of {errorRof} such a rule must be at least as great as the Bayes probability of {errorRˆ}{\textbackslash}textbackslashast–the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in {theM}-category case {thatRˆ}{\textbackslash}textbackslashast {\textbackslash}textbackslashleq R {\textbackslash}textbackslashleq Rˆ{\textbackslash}textbackslashast(2 –{MRˆ}{\textbackslash}textbackslashast/(M-1)), where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
	pages = {21--27},
	number = {1},
	journaltitle = {{IEEE} Transactions on Information Theory},
	author = {Cover, T. and Hart, P.},
	date = {1967-01},
	keywords = {[introduction], algo : knn},
	annotation = {Conference Name: {IEEE} Transactions on Information Theory},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Celian\\Zotero\\storage\\3G8JSERH\\1053964.html:text/html;Version soumise:C\:\\Users\\Celian\\Zotero\\storage\\D7E3NLPC\\Cover et Hart - 1967 - Nearest neighbor pattern classification.pdf:application/pdf}
}

@inproceedings{davis_information-theoretic_2007,
	location = {Corvalis, Oregon},
	title = {Information-theoretic metric learning},
	isbn = {978-1-59593-793-3},
	url = {http://portal.acm.org/citation.cfm?doid=1273496.1273523},
	doi = {10.1145/1273496.1273523},
	abstract = {In this paper, we present an information-theoretic approach to learning a Mahalanobis distance function. We formulate the problem as that of minimizing the differential relative entropy between two multivariate Gaussians under constraints on the distance function. We express this problem as a particular Bregman optimization problem—that of minimizing the {LogDet} divergence subject to linear constraints. Our resulting algorithm has several advantages over existing methods. First, our method can handle a wide variety of constraints and can optionally incorporate a prior on the distance function. Second, it is fast and scalable. Unlike most existing methods, no eigenvalue computations or semi-deﬁnite programming are required. We also present an online version and derive regret bounds for the resulting algorithm. Finally, we evaluate our method on a recent error reporting system for software called Clarify, in the context of metric learning for nearest neighbor classiﬁcation, as well as on standard data sets.},
	pages = {209--216},
	booktitle = {Proceedings of the 24th international conference on Machine learning - {ICML} '07},
	publisher = {{ACM} Press},
	author = {Davis, Jason V. and Kulis, Brian and Jain, Prateek and Sra, Suvrit and Dhillon, Inderjit S.},
	urldate = {2021-04-19},
	date = {2007},
	langid = {english},
	keywords = {[introduction], distance methods : mahanalobis, metric learning},
	file = {Davis et al. - 2007 - Information-theoretic metric learning.pdf:C\:\\Users\\Celian\\Zotero\\storage\\2L3P8RJ4\\Davis et al. - 2007 - Information-theoretic metric learning.pdf:application/pdf}
}

@article{dudani_distance-weighted_1976,
	title = {The Distance-Weighted k-Nearest-Neighbor Rule},
	volume = {{SMC}-6},
	issn = {2168-2909},
	doi = {10.1109/TSMC.1976.5408784},
	abstract = {Among the simplest and most intuitively appealing classes of nonprobabilistic classification procedures are those that weight the evidence of nearby sample observations most heavily. More specifically, one might wish to weight the evidence of a neighbor close to an unclassified observation more heavily than the evidence of another neighbor which is at a greater distance from the unclassified observation. One such classification rule is described which makes use of a neighbor weighting function for the purpose of assigning a class to an unclassified sample. The admissibility of such a rule is also considered.},
	pages = {325--327},
	number = {4},
	journaltitle = {{IEEE} Transactions on Systems, Man, and Cybernetics},
	author = {Dudani, S. A.},
	date = {1976-04},
	keywords = {[experiment], algo : wk-nn, distance methods : weighted},
	annotation = {Conference Name: {IEEE} Transactions on Systems, Man, and Cybernetics},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Celian\\Zotero\\storage\\9C4NXCZV\\5408784.html:text/html}
}

@article{fernandez_smote_2018,
	title = {{SMOTE} for Learning from Imbalanced Data: Progress and Challenges, Marking the 15-year Anniversary},
	volume = {61},
	rights = {Copyright (c)},
	issn = {1076-9757},
	url = {https://www.jair.org/index.php/jair/article/view/11192},
	doi = {10.1613/jair.1.11192},
	shorttitle = {{SMOTE} for Learning from Imbalanced Data},
	pages = {863--905},
	journaltitle = {Journal of Artificial Intelligence Research},
	author = {Fernandez, Alberto and Garcia, Salvador and Herrera, Francisco and Chawla, Nitesh V.},
	urldate = {2021-04-19},
	date = {2018-04},
	langid = {english},
	keywords = {[introduction], imbalanced data, sampling method : {SMOTE}},
	file = {Full Text PDF:C\:\\Users\\Celian\\Zotero\\storage\\3BJUKRP5\\Fernandez et al. - 2018 - SMOTE for Learning from Imbalanced Data Progress .pdf:application/pdf;Snapshot:C\:\\Users\\Celian\\Zotero\\storage\\9XY2H4XE\\11192.html:text/html}
}

@book{ferri_experimental_nodate,
	title = {An experimental comparison of performance measures for classification {\textbackslash}textbar Pattern Recognition Letters},
	url = {https://dl.acm.org/doi/abs/10.1016/j.patrec.2008.08.010},
	author = {Ferri, C},
	urldate = {2021-04-19},
	keywords = {[introduction], eval mesure : {SOA}, metric learning},
	file = {An experimental comparison of performance measures for classification | Pattern Recognition Letters:C\:\\Users\\Celian\\Zotero\\storage\\NWDRU9ZT\\j.patrec.2008.08.html:text/html}
}

@article{frery_efficient_nodate,
	title = {Efficient top rank optimization with gradient boosting for supervised anomaly detection},
	abstract = {In this paper we address the anomaly detection problem in a supervised setting where positive examples might be very sparse. We tackle this task with a learning to rank strategy by optimizing a differentiable smoothed surrogate of the so-called Average Precision ({AP}). Despite its non-convexity, we show how to use it eﬃciently in a stochastic gradient boosting framework. We show that using {AP} is much better to optimize the top rank alerts than the state of the art measures. We demonstrate on anomaly detection tasks that the interest of our method is even reinforced in highly unbalanced scenarios.},
	pages = {17},
	author = {Frery, Jordan and Habrard, Amaury and Sebban, Marc and Caelen, Olivier and He-Guelton, Liyun},
	langid = {english},
	keywords = {[introduction], anomaly detection, eval mesure : {AP} - gb optimization},
	file = {Frery et al. - Efficient top rank optimization with gradient boos.pdf:C\:\\Users\\Celian\\Zotero\\storage\\NB3MVAWC\\Frery et al. - Efficient top rank optimization with gradient boos.pdf:application/pdf}
}

@book{gee_fraud_nodate,
	title = {Fraud and Fraud Detection: A Data Analytics Approach, + Website {\textbackslash}textbar Wiley},
	url = {https://www.wiley.com/en-us/Fraud+and+Fraud+Detection%3A+A+Data+Analytics+Approach%2C+%2B+Website-p-9781118779651},
	shorttitle = {Fraud and Fraud Detection},
	abstract = {Detect fraud faster—no matter how well hidden—with {IDEA} automation Fraud and Fraud Detection takes an advanced approach to fraud management, providing step-by-step guidance on automating detection and forensics using {CaseWares} {IDEA} software. The book begins by reviewing the major types of fraud, then details the specific computerized tests that can detect them. Readers will learn to use complex data analysis techniques, including automation scripts, allowing easier and more sensitive detection of anomalies that require further review. The companion website provides access to a demo version of {IDEA}, along with sample scripts that allow readers to immediately test the procedures from the book. Business systems electronic databases have grown tremendously with the rise of big data, and will continue to increase at significant rates. Fraudulent transactions are easily hidden in these enormous datasets, but Fraud and Fraud Detection helps readers gain the data analytics skills that can bring these anomalies to light. Step-by-step instruction and practical advice provide the specific abilities that will enhance the audit and investigation process. Readers will learn to: Understand the different areas of fraud and their specific detection methods Identify anomalies and risk areas using computerized techniques Develop a step-by-step plan for detecting fraud through data analytics Utilize {IDEA} software to automate detection and identification procedures The delineation of detection techniques for each type of fraud makes this book a must-have for students and new fraud prevention professionals, and the step-by-step guidance to automation and complex analytics will prove useful for even experienced examiners. With datasets growing exponentially, increasing both the speed and sensitivity of detection helps fraud professionals stay ahead of the game. Fraud and Fraud Detection is a guide to more efficient, more effective fraud identification.},
	author = {Gee, S.},
	urldate = {2021-04-19},
	langid = {english},
	note = {Publication Title: Wiley.com},
	keywords = {eval mesure : F1, imbalanced data},
	file = {Snapshot:C\:\\Users\\Celian\\Zotero\\storage\\UNKL7D2T\\Fraud+and+Fraud+Detection+A+Data+Analytics+Approach,+++Website-p-9781118779651.html:text/html}
}

@article{goodfellow_generative_2014,
	title = {Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	journaltitle = {{arXiv}:1406.2661 [cs, stat]},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	urldate = {2021-04-19},
	date = {2014-06},
	keywords = {[introduction], algo : generative model},
	annotation = {{arXiv}: 1406.2661},
	file = {arXiv Fulltext PDF:C\:\\Users\\Celian\\Zotero\\storage\\LF88ZEA4\\Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Celian\\Zotero\\storage\\Z4NXF4NU\\1406.html:text/html}
}

@article{hajizadeh_nearest_2014,
	title = {Nearest Neighbor Classification with Locally Weighted Distance for Imbalanced Data},
	volume = {3},
	issn = {20103743},
	url = {http://www.ijcce.org/index.php?m=content&c=index&a=show&catid=39&id=352},
	doi = {10.7763/IJCCE.2014.V3.296},
	abstract = {The datasets used in many real applications are highly imbalanced which makes classification problem hard. Classifying the minor class instances is difficult due to bias of the classifier output to the major classes. Nearest neighbor is one of the most popular and simplest classifiers with good performance on many datasets. However, correctly classifying the minor class is commonly sacrificed to achieve a better performance on others. This paper is aimed to improve the performance of nearest neighbor in imbalanced domains, without disrupting the real data distribution. Prototype-weighting is proposed, here, to locally adapting the distances to increase the chance of prototypes from minor class to be the nearest neighbor of a query instance. The objective function is, here, G-mean and optimization process is performed using gradient ascent method. Comparing the experimental results, our proposed method significantly outperformed similar works on 24 standard data sets.},
	pages = {81--86},
	number = {2},
	journaltitle = {International Journal of Computer and Communication Engineering},
	author = {Hajizadeh, Zahra and Taheri, Mohammad and Jahromi, Mansoor Zolghadri},
	urldate = {2021-04-19},
	date = {2014},
	langid = {english},
	keywords = {[experiment], distance methods : weighted, eval mesure : accuracy - optimization, imbalanced data},
	file = {Hajizadeh et al. - 2014 - Nearest Neighbor Classification with Locally Weigh.pdf:C\:\\Users\\Celian\\Zotero\\storage\\6W76F9GS\\Hajizadeh et al. - 2014 - Nearest Neighbor Classification with Locally Weigh.pdf:application/pdf}
}

@inproceedings{han_borderline-smote_2005,
	location = {Berlin, Heidelberg},
	title = {Borderline-{SMOTE}: A New Over-Sampling Method in Imbalanced Data Sets Learning},
	isbn = {978-3-540-31902-3},
	doi = {10.1007/11538059_91},
	series = {Lecture Notes in Computer Science},
	shorttitle = {Borderline-{SMOTE}},
	abstract = {In recent years, mining with imbalanced data sets receives more and more attentions in both theoretical and practical aspects. This paper introduces the importance of imbalanced data sets and their broad application domains in data mining, and then summarizes the evaluation metrics and the existing methods to evaluate and solve the imbalance problem. Synthetic minority over-sampling technique ({SMOTE}) is one of the over-sampling methods addressing this problem. Based on {SMOTE} method, this paper presents two new minority over-sampling methods, borderline-{SMOTE}1 and borderline-{SMOTE}2, in which only the minority examples near the borderline are over-sampled. For the minority class, experiments show that our approaches achieve better {TP} rate and F-value than {SMOTE} and random over-sampling methods.},
	pages = {878--887},
	booktitle = {Advances in Intelligent Computing},
	publisher = {Springer},
	author = {Han, Hui and Wang, Wen-Yuan and Mao, Bing-Huan},
	editor = {Huang, De-Shuang and Zhang, Xiao-Ping and Huang, Guang-Bin},
	date = {2005},
	langid = {english},
	keywords = {[experiment], imbalanced data, sampling method : Boderline-{SMOTE}},
	file = {Springer Full Text PDF:C\:\\Users\\Celian\\Zotero\\storage\\JAYGR3L6\\Han et al. - 2005 - Borderline-SMOTE A New Over-Sampling Method in Im.pdf:application/pdf}
}

@inproceedings{haibo_he_adasyn_2008,
	location = {Hong Kong, China},
	title = {{ADASYN}: Adaptive synthetic sampling approach for imbalanced learning},
	isbn = {978-1-4244-1820-6},
	url = {http://ieeexplore.ieee.org/document/4633969/},
	doi = {10.1109/IJCNN.2008.4633969},
	shorttitle = {{ADASYN}},
	abstract = {This paper presents a novel adaptive synthetic ({ADASYN}) sampling approach for learning from imbalanced data sets. The essential idea of {ADASYN} is to use a weighted distribution for different minority class examples according to their level of difﬁculty in learning, where more synthetic data is generated for minority class examples that are harder to learn compared to those minority examples that are easier to learn. As a result, the {ADASYN} approach improves learning with respect to the data distributions in two ways: (1) reducing the bias introduced by the class imbalance, and (2) adaptively shifting the classiﬁcation decision boundary toward the difﬁcult examples. Simulation analyses on several machine learning data sets show the effectiveness of this method across ﬁve evaluation metrics.},
	pages = {1322--1328},
	booktitle = {2008 {IEEE} International Joint Conference on Neural Networks ({IEEE} World Congress on Computational Intelligence)},
	publisher = {{IEEE}},
	author = {{Haibo He} and {Yang Bai} and Garcia, Edwardo A. and {Shutao Li}},
	urldate = {2021-04-19},
	date = {2008-06},
	langid = {english},
	keywords = {[experiment], imbalanced data, sampling method : {ADASYN}},
	file = {Haibo He et al. - 2008 - ADASYN Adaptive synthetic sampling approach for i.pdf:C\:\\Users\\Celian\\Zotero\\storage\\NZH45W2S\\Haibo He et al. - 2008 - ADASYN Adaptive synthetic sampling approach for i.pdf:application/pdf}
}

@article{kontorovich_active_nodate,
	title = {Active Nearest-Neighbor Learning in Metric Spaces},
	abstract = {We propose a pool-based non-parametric active learning algorithm for general metric spaces, called {MArgin} Regularized Metric Active Nearest Neighbor ({MARMANN}), which outputs a nearestneighbor classiﬁer. We give prediction error guarantees that depend on the noisy-margin properties of the input sample, and are competitive with those obtained by previously proposed passive learners. We prove that the label complexity of {MARMANN} is signiﬁcantly lower than that of any passive learner with similar error guarantees. {MARMANN} is based on a generalized sample compression scheme, and a new label-efﬁcient active model-selection procedure.},
	pages = {38},
	author = {Kontorovich, Aryeh and Sabato, Sivan and Urner, Ruth},
	langid = {english},
	keywords = {[introduction], algo : knn, distance methods},
	file = {Kontorovich et al. - Active Nearest-Neighbor Learning in Metric Spaces.pdf:C\:\\Users\\Celian\\Zotero\\storage\\MNKBF8P3\\Kontorovich et al. - Active Nearest-Neighbor Learning in Metric Spaces.pdf:application/pdf}
}

@article{kontorovich_bayes_2018,
	title = {A Bayes consistent 1-{NN} classifier},
	url = {http://arxiv.org/abs/1407.0208},
	abstract = {We show that a simple modification of the 1-nearest neighbor classifier yields a strongly Bayes consistent learner. Prior to this work, the only strongly Bayes consistent proximity-based method was the k-nearest neighbor classifier, for k growing appropriately with sample size. We will argue that a margin-regularized 1-{NN} enjoys considerable statistical and algorithmic advantages over the k-{NN} classifier. These include user-friendly finite-sample error bounds, as well as time- and memory-efficient learning and test-point evaluation algorithms with a principled speed-accuracy tradeoff. Encouraging empirical results are reported.},
	journaltitle = {{arXiv}:1407.0208 [cs, stat]},
	author = {Kontorovich, Aryeh and Weiss, Roi},
	urldate = {2021-04-19},
	date = {2018-08},
	keywords = {[introduction], algo : knn, distance methods},
	annotation = {{arXiv}: 1407.0208},
	file = {arXiv Fulltext PDF:C\:\\Users\\Celian\\Zotero\\storage\\KE3CVWK8\\Kontorovich et Weiss - 2018 - A Bayes consistent 1-NN classifier.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Celian\\Zotero\\storage\\9MTVAD88\\1407.html:text/html}
}

@inproceedings{liu_class_2011,
	location = {Berlin, Heidelberg},
	title = {Class Confidence Weighted {kNN} Algorithms for Imbalanced Data Sets},
	isbn = {978-3-642-20847-8},
	doi = {10.1007/978-3-642-20847-8_29},
	series = {Lecture Notes in Computer Science},
	abstract = {In this paper, a novel k-nearest neighbors ({kNN}) weighting strategy is proposed for handling the problem of class imbalance. When dealing with highly imbalanced data, a salient drawback of existing {kNN} algorithms is that the class with more frequent samples tends to dominate the neighborhood of a test instance in spite of distance measurements, which leads to suboptimal classification performance on the minority class. To solve this problem, we propose {CCW} (class confidence weights) that uses the probability of attribute values given class labels to weight prototypes in {kNN}. The main advantage of {CCW} is that it is able to correct the inherent bias to majority class in existing {kNN} algorithms on any distance measurement. Theoretical analysis and comprehensive experiments confirm our claims.},
	pages = {345--356},
	booktitle = {Advances in Knowledge Discovery and Data Mining},
	publisher = {Springer},
	author = {Liu, Wei and Chawla, Sanjay},
	editor = {Huang, Joshua Zhexue and Cao, Longbing and Srivastava, Jaideep},
	date = {2011},
	langid = {english},
	keywords = {distance methods : distribution based, imbalanced data},
	file = {Springer Full Text PDF:C\:\\Users\\Celian\\Zotero\\storage\\CD77YHLF\\Liu et Chawla - 2011 - Class Confidence Weighted kNN Algorithms for Imbal.pdf:application/pdf}
}

@incollection{von_luxburg_distance-based_2003,
	location = {Berlin, Heidelberg},
	title = {Distance-Based Classification with Lipschitz Functions},
	volume = {2777},
	isbn = {978-3-540-40720-1 978-3-540-45167-9},
	url = {http://link.springer.com/10.1007/978-3-540-45167-9_24},
	abstract = {The goal of this article is to develop a framework for large margin classiﬁcation in metric spaces. We want to ﬁnd a generalization of linear decision functions for metric spaces and deﬁne a corresponding notion of margin such that the decision function separates the training points with a large margin. It will turn out that using Lipschitz functions as decision functions, the inverse of the Lipschitz constant can be interpreted as the size of a margin. In order to construct a clean mathematical setup we isometrically embed the given metric space into a Banach space and the space of Lipschitz functions into its dual space. To analyze the resulting algorithm, we prove several representer theorems. They state that there always exist solutions of the Lipschitz classiﬁer which can be expressed in terms of distance functions to training points. We provide generalization bounds for Lipschitz classiﬁers in terms of the Rademacher complexities of some Lipschitz function classes. The generality of our approach can be seen from the fact that several well-known algorithms are special cases of the Lipschitz classiﬁer, among them the support vector machine, the linear programming machine, and the 1-nearest neighbor classiﬁer.},
	pages = {314--328},
	booktitle = {Learning Theory and Kernel Machines},
	publisher = {Springer Berlin Heidelberg},
	author = {von Luxburg, Ulrike and Bousquet, Olivier},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Schölkopf, Bernhard and Warmuth, Manfred K.},
	urldate = {2021-04-19},
	date = {2003},
	langid = {english},
	doi = {10.1007/978-3-540-45167-9_24},
	keywords = {[introduction], algo : knn, distance methods},
	annotation = {Series Title: Lecture Notes in Computer Science},
	file = {von Luxburg et Bousquet - 2003 - Distance-Based Classification with Lipschitz Funct.pdf:C\:\\Users\\Celian\\Zotero\\storage\\8FIPRCTR\\von Luxburg et Bousquet - 2003 - Distance-Based Classification with Lipschitz Funct.pdf:application/pdf}
}

@article{rijsbergen_information_1979,
	title = {Information Retrieval, 2nd ed. C.J. Van Rijsbergen. London: Butterworths; 1979: 208 pp. Price: \$32.50},
	volume = {30},
	issn = {00028231, 10974571},
	url = {http://doi.wiley.com/10.1002/asi.4630300621},
	doi = {10.1002/asi.4630300621},
	shorttitle = {Information Retrieval, 2nd ed. C.J. Van Rijsbergen. London},
	pages = {374--375},
	number = {6},
	journaltitle = {Journal of the American Society for Information Science},
	author = {Rijsbergen, {JCV}},
	urldate = {2021-04-19},
	date = {1979-11},
	langid = {english},
	keywords = {eval mesure : F1, information retrieval},
	file = {rijsbergen79_infor_retriev.pdf:C\:\\Users\\Celian\\Zotero\\storage\\F2GB2YPN\\rijsbergen79_infor_retriev.pdf:application/pdf}
}

@article{sajjadi_assessing_2018,
	title = {Assessing Generative Models via Precision and Recall},
	url = {http://arxiv.org/abs/1806.00035},
	abstract = {Recent advances in generative modeling have led to an increased interest in the study of statistical divergences as means of model comparison. Commonly used evaluation methods, such as the Frechet Inception Distance ({FID}), correlate well with the perceived quality of samples and are sensitive to mode dropping. However, these metrics are unable to distinguish between different failure cases since they only yield one-dimensional scores. We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions. The proposed notion is intuitive, retains desirable properties, and naturally leads to an efficient algorithm that can be used to evaluate generative models. We relate this notion to total variation as well as to recent evaluation metrics such as Inception Score and {FID}. To demonstrate the practical utility of the proposed approach we perform an empirical study on several variants of Generative Adversarial Networks and Variational Autoencoders. In an extensive set of experiments we show that the proposed metric is able to disentangle the quality of generated samples from the coverage of the target distribution.},
	journaltitle = {{arXiv}:1806.00035 [cs, stat]},
	author = {Sajjadi, Mehdi S. M. and Bachem, Olivier and Lucic, Mario and Bousquet, Olivier and Gelly, Sylvain},
	urldate = {2021-04-19},
	date = {2018-10},
	keywords = {[introduction], algo : generative model, eval mesure},
	annotation = {{arXiv}: 1806.00035},
	annotation = {Comment: {NIPS} 2018},
	file = {arXiv Fulltext PDF:C\:\\Users\\Celian\\Zotero\\storage\\67YGV973\\Sajjadi et al. - 2018 - Assessing Generative Models via Precision and Reca.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Celian\\Zotero\\storage\\RVAJYKJG\\1806.html:text/html}
}

@inproceedings{steck_hinge_2007,
	title = {Hinge Rank Loss and the Area Under the {ROC} Curve},
	doi = {10.1007/978-3-540-74958-5_33},
	abstract = {In ranking as well as in classification problems, the Area under the {ROC} Curve ({AUC}), or the equivalent Wilcoxon-Mann-Whitney statistic, has recently attracted a lot of attention. We show that the {AUC} can be lower bounded based on the hinge-rank-loss, which simply is the rank-version of the standard (parametric) hinge loss. This bound is asymptotically tight. Our experiments indicate that optimizing the (standard) hinge loss typically is an accurate approximation to optimizing the hinge rank loss, especially when using affine transformations of the data, like e.g. in ellipsoidal machines. This explains for the first time why standard training of support vector machines approximately maximizes the {AUC}, which has indeed been observed in many experiments in the literature.},
	booktitle = {{ECML}},
	author = {Steck, H.},
	date = {2007},
	keywords = {[introduction], eval mesure : F1},
	file = {Texte intégral:C\:\\Users\\Celian\\Zotero\\storage\\8QAIQ8PH\\Steck - 2007 - Hinge Rank Loss and the Area Under the ROC Curve.pdf:application/pdf}
}

@inproceedings{tomek_two_1976,
	title = {Two Modifications of {CNN}},
	doi = {10.1109/tsmc.1976.4309452},
	abstract = {Semantic Scholar extracted view of "Two Modifications of {CNN}" by I. Tomek},
	author = {Tomek, I.},
	date = {1976},
	keywords = {[experiment], [introduction], algo : cnn, sampling method : Tomek’s link}
}

@article{weinberger_distance_nodate,
	title = {Distance Metric Learning for Large Margin Nearest Neighbor Classiﬁcation},
	abstract = {The accuracy of k-nearest neighbor ({kNN}) classiﬁcation depends signiﬁcantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Mahalanobis distance metric for {kNN} classiﬁcation from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes {kNN} classiﬁcation using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. As in support vector machines ({SVMs}), the margin criterion leads to a convex optimization based on the hinge loss. Unlike learning in {SVMs}, however, our approach requires no modiﬁcation or extension for problems in multiway (as opposed to binary) classiﬁcation. In our framework, the Mahalanobis distance metric is obtained as the solution to a semideﬁnite program. On several data sets of varying size and difﬁculty, we ﬁnd that metrics trained in this way lead to signiﬁcant improvements in {kNN} classiﬁcation. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner.},
	pages = {38},
	author = {Weinberger, Kilian Q and Saul, Lawrence K},
	langid = {english},
	keywords = {[experiment], [introduction], distance methods : mahanalobis, eval mesure : F1 - optimization, metric learning},
	file = {Weinberger et Saul - Distance Metric Learning for Large Margin Nearest .pdf:C\:\\Users\\Celian\\Zotero\\storage\\QTU98ACP\\Weinberger et Saul - Distance Metric Learning for Large Margin Nearest .pdf:application/pdf}
}

@book{wilson_asymptotic_nodate,
	title = {Asymptotic Properties of Nearest Neighbor Rules Using Edited Data {\textbackslash}textbar {IEEE} Journals \& Magazine {\textbackslash}textbar {IEEE} Xplore},
	url = {https://ieeexplore.ieee.org/document/4309137},
	author = {Wilson, D.L.},
	urldate = {2021-04-19},
	keywords = {sampling method : enn},
	file = {Asymptotic Properties of Nearest Neighbor Rules Using Edited Data | IEEE Journals & Magazine | IEEE Xplore:C\:\\Users\\Celian\\Zotero\\storage\\QIEUHH8K\\4309137.html:text/html}
}

@inproceedings{zhang_positive-biased_2013,
	location = {Berlin, Heidelberg},
	title = {A Positive-biased Nearest Neighbour Algorithm for Imbalanced Classification},
	isbn = {978-3-642-37456-2},
	doi = {10.1007/978-3-642-37456-2_25},
	series = {Lecture Notes in Computer Science},
	abstract = {The k nearest neighbour ({kNN}) algorithm classifies a query instance to the most frequent class among its k nearest neighbours in the training instance space. For imbalanced class distribution where positive training instances are rare, a query instance is often overwhelmed by negative instances in its neighbourhood and likely to be classified to the negative majority class. In this paper we propose a Positive-biased Nearest Neighbour ({PNN}) algorithm, where the local neighbourhood of query instances is dynamically formed and classification decision is carefully adjusted based on class distribution in the local neighbourhood. Extensive experiments on real-world imbalanced datasets show that {PNN} has good performance for imbalanced classification. {PNN} often outperforms recent {kNN}-based imbalanced classification algorithms while significantly reducing their extra computation cost.},
	pages = {293--304},
	booktitle = {Advances in Knowledge Discovery and Data Mining},
	publisher = {Springer},
	author = {Zhang, Xiuzhen and Li, Yuxuan},
	editor = {Pei, Jian and Tseng, Vincent S. and Cao, Longbing and Motoda, Hiroshi and Xu, Guandong},
	date = {2013},
	langid = {english},
	keywords = {algo : kpnn, distance methods : distribution based},
	file = {Springer Full Text PDF:C\:\\Users\\Celian\\Zotero\\storage\\9PJPWVTS\\Zhang et Li - 2013 - A Positive-biased Nearest Neighbour Algorithm for .pdf:application/pdf}
}

@book{zhang_krnn_nodate,
	title = {{KRNN} {\textbackslash}textbar Pattern Recognition},
	url = {https://dl.acm.org/doi/abs/10.1016/j.patcog.2016.08.023},
	author = {Zhang, X.},
	urldate = {2021-04-19},
	keywords = {[experiment], distance methods : distribution based, algo : krnn},
	file = {KRNN | Pattern Recognition:C\:\\Users\\Celian\\Zotero\\storage\\R5RY7L8W\\j.patcog.2016.08.html:text/html}
}